{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Data Gathering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import pytrends\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Dominique\\\\Documents\\\\Work\\\\ADB\\\\Nowcasting\\\\Data'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Google Trends data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few helper functions, <code>topic_mapper()</code> and <code>scrape_trends</code> for obtaining Google Trends search data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_mapper(terms, types):\n",
    "    \"\"\"\n",
    "    Map overarching topics for given search terms as well as associated suggestions.\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    titles = []\n",
    "    trend = TrendReq()\n",
    "    for term in terms:\n",
    "        l = trend.suggestions(term)\n",
    "        keywords.append([\n",
    "            d['mid'] for d in l\n",
    "            if d['type'] in types\n",
    "        ])\n",
    "        titles.append([\n",
    "            d['title'] for d in l\n",
    "            if d['type'] in types\n",
    "        ])\n",
    "    keywords = [key for sublist in keywords for key in sublist]\n",
    "    titles = [key for sublist in titles for key in sublist]\n",
    "    mapping = dict(zip(keywords, titles))\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_trends(countries, mapping, time):\n",
    "    \"\"\"\n",
    "    Scrape search volume intensity for specified countries based on topic mapping. Save results as .csv file. \n",
    "    \"\"\"\n",
    "    trend = TrendReq()\n",
    "\n",
    "    for country in tqdm(countries):\n",
    "        dataset = []\n",
    "        for x, v in tqdm(mapping.items()):\n",
    "            #     for x in tqdm(range(0,len(keywords))):\n",
    "            #         kw = [keywords[x]]\n",
    "            kw = [str(x)]\n",
    "            trend.build_payload(kw_list=kw, timeframe=time, geo=country)\n",
    "            data = trend.interest_over_time()\n",
    "            if not data.empty:\n",
    "                data = data.drop(labels=['isPartial'], axis='columns')\n",
    "                dataset.append(data)\n",
    "        \n",
    "        result = pd.concat(dataset, axis=1)\n",
    "        result.rename(columns=mapping, inplace=True)\n",
    "        file_name = str(country) + \"_search_trends.csv\"\n",
    "        result.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the search terms that we are interested in. The <code>topic_mapper()</code> function automatically gets Google's suggestions, which are categorized into types. For this research, we use the following types:\n",
    "\n",
    "1. **Topic**: The overarching search term, which has a non-intuitive tag (i.e. <code>/m/0d7pp</code> is the topic for all 'credit card'-related searches).\n",
    "2. **Transportation mode**: This is relevant for flights and automotives, since we are not interested in, say, scientific research about automotive or flights.\n",
    "3. **Website**: This is to capture search data for travel-related websites such as Airbnb, booking.com, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\n",
    "    'Credit card', 'Banking', 'Online hotel reservations', 'Restaurant',\n",
    "    'Flights', 'Mobile phone', 'Automotive industry', 'Travel'\n",
    "]\n",
    "types = ['Topic', 'Transportation mode', 'Website']\n",
    "\n",
    "mapping = topic_mapper(terms, types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <code> scrape_trends()</code> function we defined above, this snippet scrapes and downloads the search trend information for the specified countries. We use only information from 2011 onwards, as the search data are more reliable from that point onwards since Google changed their data collection system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['AU', 'CN', 'ID', 'IN', 'JP', 'KR', 'MM', 'MY', 'NZ', 'PH', 'SG', 'TH', 'TW', 'VN']\n",
    "time = '2018-01-01 2020-07-17'\n",
    "\n",
    "scrape_trends(countries, mapping, time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI\n",
    "Monthly IHS Markit data from CEIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = pd.read_excel(path + '\\iso_codes.xlsx', header=0)\n",
    "iso_dict = dict(zip(iso['Alpha-3 code'], iso['Alpha-2 code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming to uniform ISO 3-codes\n",
    "for key, value in iso_dict.items():\n",
    "    if os.path.isfile(key + ' PMI.xlsx'):\n",
    "        os.rename(key + ' PMI.xlsx', value + '_pmi.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['AU', 'CN', 'ID', 'IN', 'JP', 'KR', 'MM', 'MY', 'NZ', 'PH', 'SG', 'TH', 'TW', 'VN']\n",
    "df1 = pd.DataFrame(columns = ['date', 'pmi', 'country'])\n",
    "\n",
    "for i in countries:\n",
    "    file = 'pmi/' + i + '_pmi.xlsx'\n",
    "    \n",
    "    df = pd.read_excel(\n",
    "        file,\n",
    "        skiprows=range(1, 26),\n",
    "        names=['date', 'pmi'],\n",
    "        usecols=[0, 1],\n",
    "        parse_dates=True)\n",
    "\n",
    "    df['country'] = i\n",
    "\n",
    "    df1 = pd.concat([df1, df])\n",
    "    \n",
    "df1.to_csv(\"pmi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends\n",
    "Weekly Google Trends Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['AU', 'CN', 'ID', 'IN', 'JP', 'KR', 'MM', 'MY', 'NZ', 'PH', 'SG', 'TH', 'TW', 'VN']\n",
    "df1 = pd.DataFrame(columns = ['date', 'country', 'credit', 'banking', 'hotels', 'cars', 'phones', 'travels', 'restaurants'])\n",
    "\n",
    "for i in countries:\n",
    "    file = 'search_trends/' + i + '_search_trends.csv'\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    df['country'] = i\n",
    "    df['credit'] = df[['Credit card', 'Cashback reward program']].mean(axis=1)\n",
    "    df['banking'] = df[['Investment banking', 'Mobile banking', 'Online banking']].mean(axis=1)\n",
    "    df['hotels'] = df[['Online hotel reservations', 'Hotels.com']].mean(axis=1) \n",
    "    df['cars'] = df[['Automotive industry', 'Car']].mean(axis=1)\n",
    "    df['phones'] = df[['Mobile phone', 'Mobile phone accessories']].mean(axis=1)\n",
    "    df['travels'] = df[['Google Flights', 'Travel', 'Tourism', 'Travel visa']].mean(axis=1)\n",
    "    df['restaurants'] = df[['Restaurant', 'Steakhouse']].mean(axis=1)\n",
    "\n",
    "    df = df[['date', 'country', 'credit', 'banking', 'hotels', 'cars', 'phones', 'travels', 'restaurants']]\n",
    "    \n",
    "    df1 = pd.concat([df1, df])\n",
    "    df1['date'] = pd.to_datetime(df1['date'])\n",
    "    \n",
    "df1.to_csv('search_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bond yield\n",
    "Short term bond yields from national sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = pd.read_excel(path + '\\iso_codes.xlsx', header=0)\n",
    "iso_dict = dict(zip(iso['Country'], iso['Alpha-2 code']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('stbond/stbond.xlsx',\n",
    "                   skiprows=range(2, 26),\n",
    "                   parse_dates=True,\n",
    "                   header=1)\n",
    "df.rename(columns={\n",
    "    'Region': 'date',\n",
    "    'South Korea': 'Korea (the Republic of)',\n",
    "    'Taiwan': 'Taiwan (Province of China)'\n",
    "}, inplace=True)\n",
    "df.rename(columns=iso_dict, inplace=True)\n",
    "df = df.melt(id_vars='date', value_name='stbond')\n",
    "df.rename(columns={'variable':'country'}, inplace=True)\n",
    "df.to_csv('stbond.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P/E ratio\n",
    "Price-equity ratios from national sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('pe_ratio/pe_ratio.xlsx',\n",
    "                   skiprows=range(2, 26),\n",
    "                   parse_dates=True,\n",
    "                   header=1)\n",
    "\n",
    "df.rename(columns={\n",
    "    'Region': 'date',\n",
    "    'South Korea': 'Korea (the Republic of)',\n",
    "    'Taiwan': 'Taiwan (Province of China)'\n",
    "}, inplace=True)\n",
    "\n",
    "df.rename(columns=iso_dict, inplace=True)\n",
    "df.drop(columns='Hong Kong SAR (China)', inplace=True)\n",
    "df = df.melt(id_vars='date', value_name='pe_ratio')\n",
    "df.rename(columns={'variable':'country'}, inplace=True)\n",
    "df.to_csv('pe_ratio.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir() if x.endswith('.csv')]\n",
    "name_list = []\n",
    "\n",
    "for i in files:\n",
    "    name = i.split(sep=\".\")[0]\n",
    "    globals()[name] = pd.read_csv(i, parse_dates=[0])\n",
    "    globals()[name]['month'] = globals()[name]['date'].dt.month\n",
    "    globals()[name]['year'] = globals()[name]['date'].dt.year\n",
    "    name_list.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "merged = reduce(lambda x,y: pd.merge(x,y, on=['month','year','country'], how='left'), [search_trends, pmi,  pe_ratio, stbond])\n",
    "\n",
    "merged = pd.concat([merged.iloc[:,0:-4], merged.iloc[:,-1], merged.iloc[:,-3]], axis=1)\n",
    "\n",
    "merged.rename(columns={'date_y': 'mdate', 'date_x': 'wdate'}, inplace=True)\n",
    "merged = merged[[\n",
    "    'mdate', 'wdate', 'month', 'year', 'country', 'pmi', 'credit',\n",
    "    'banking', 'hotels', 'cars', 'phones', 'travels', 'restaurants', 'stbond', 'pe_ratio'\n",
    "]]\n",
    "\n",
    "merged.dropna(subset=['pmi', 'credit', 'banking','hotels','cars','phones','travels','restaurants'], inplace=True)\n",
    "\n",
    "merged.drop_duplicates(subset=['pmi', 'credit'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('monthly_nc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
